{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Sentiment Transfer\n",
    "\n",
    "This motivation of this project is to test whether sentiment can be changed as a tunable parameter to auto-generate review text with the same content but different sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_review_and_rating(json_text):\n",
    "    review_dict = json.loads(json_text)    \n",
    "    return review_dict['reviewText'], review_dict['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_and_ratings(reviews_filepath):\n",
    "    review_texts = list()\n",
    "    ratings = list()\n",
    "    with open(reviews_filepath) as reviews_file:\n",
    "        for line in reviews_file:\n",
    "            review_text, rating = convert_json_to_review_and_rating(line)\n",
    "            review_texts.append(review_text)\n",
    "            ratings.append(int(rating))\n",
    "            \n",
    "    return review_texts, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(word):\n",
    "    return word\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_indexed_word_sequences(review_texts):\n",
    "    vocab = list()\n",
    "    word_indices = dict()\n",
    "    indexed_sequences = list()\n",
    "    word_index = 1\n",
    "    \n",
    "    for review_text in review_texts:\n",
    "        review_text = clean_sentence(review_text)\n",
    "        tokens = spacy_nlp.tokenizer(review_text)\n",
    "        indexed_sequence = list()\n",
    "        for token in tokens:\n",
    "            token = clean_word(token)\n",
    "            if token not in word_indices:\n",
    "                vocab.append(token)\n",
    "                word_indices[token] = word_index\n",
    "                indexed_sequence.append(word_index)\n",
    "                word_index += 1\n",
    "            else:\n",
    "                indexed_sequence.append(word_indices[token])\n",
    "        indexed_sequences.append(np.asarray(indexed_sequence))\n",
    "        \n",
    "    return vocab, word_indices, indexed_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_filepath = \"data/reviews_electronics_tiny.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 500\n"
     ]
    }
   ],
   "source": [
    "review_texts, ratings = get_reviews_and_ratings(reviews_filepath)\n",
    "review_texts, ratings = shuffle(review_texts, ratings)\n",
    "print(len(review_texts), len(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, word_indices, indexed_sequences = texts_to_indexed_word_sequences(review_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE:  101001\n",
      "EMBEDDING_SIZE:  300\n",
      "MAX_SEQUENCE_LENGTH:  94\n",
      "NUM_CLASSES:  5\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "print(\"VOCAB_SIZE: \", VOCAB_SIZE)\n",
    "\n",
    "EMBEDDING_SIZE = 300\n",
    "print(\"EMBEDDING_SIZE: \", EMBEDDING_SIZE)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = int(median([len(sequence) for sequence in indexed_sequences]))\n",
    "print(\"MAX_SEQUENCE_LENGTH: \", MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "NUM_CLASSES = len(set(ratings))\n",
    "print(\"NUM_CLASSES: \", NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_indexed_sequences(indexed_sequences, max_sequence_length):\n",
    "    new_indexed_sequences = list()\n",
    "    for sequence in indexed_sequences:\n",
    "        if len(sequence) >= max_sequence_length:\n",
    "            new_indexed_sequences.append(sequence[:max_sequence_length])\n",
    "        else:\n",
    "            shortfall = max_sequence_length - len(sequence)\n",
    "            new_indexed_sequences.append(\n",
    "                np.pad(sequence, (0, shortfall), 'constant', \n",
    "                       constant_values=(0, 0)))\n",
    "    return np.asarray(new_indexed_sequences)\n",
    "\n",
    "# def convert_labels_to_logits(ratings, num_classes):\n",
    "#     one_hot_ratings = list()\n",
    "#     for rating in ratings:\n",
    "#         one_hot_rating = np.zeros(num_classes)\n",
    "#         one_hot_rating[rating - 1] = 1\n",
    "#         one_hot_ratings.append(one_hot_rating)\n",
    "        \n",
    "#     return np.asarray(one_hot_ratings)\n",
    "\n",
    "def tensorize_sequences_and_labels(indexed_sequences, ratings, max_sequence_length, num_classes):\n",
    "    return pad_indexed_sequences(indexed_sequences, max_sequence_length), np.asarray([ratings]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_sequences, labels = tensorize_sequences_and_labels(\n",
    "    indexed_sequences, ratings, MAX_SEQUENCE_LENGTH, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 94), (500, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_sequences.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train Generative Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sequence:  Tensor(\"input_sequence:0\", shape=(?, 94), dtype=int32)\n",
      "input_rating:  Tensor(\"input_rating:0\", shape=(?, 1), dtype=float32)\n",
      "word_embeddings:  <tf.Variable 'word_embeddings:0' shape=(101002, 300) dtype=float32_ref>\n",
      "embedded_sequence:  Tensor(\"embedded_sequence:0\", shape=(?, 94, 300), dtype=float32)\n",
      "rnn_output_states_fw:  Tensor(\"encoder_lstm/bidirectional_rnn/fw/fw/while/Exit_2:0\", shape=(?, 64), dtype=float32)\n",
      "rnn_output_states_bw:  Tensor(\"encoder_lstm/bidirectional_rnn/bw/bw/while/Exit_2:0\", shape=(?, 64), dtype=float32)\n",
      "final_lstm_output:  Tensor(\"concat:0\", shape=(?, 128), dtype=float32)\n",
      "conditioned_lstm_output:  Tensor(\"concat_1:0\", shape=(?, 129), dtype=float32)\n",
      "sequence_indices:  Tensor(\"concat_96:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "graph_1 = tf.Graph()\n",
    "current_epoch = int(time.time())\n",
    "\n",
    "with graph_1.as_default():\n",
    "\n",
    "    # One-hot encoded representation of the text\n",
    "    input_sequence = tf.placeholder(\n",
    "        tf.int32, [None, MAX_SEQUENCE_LENGTH], name=\"input_sequence\")\n",
    "    print(\"input_sequence: \", input_sequence)\n",
    "    \n",
    "    # Actual rating\n",
    "    input_rating = tf.placeholder(\n",
    "        tf.float32, [None, 1], name=\"input_rating\")\n",
    "    print(\"input_rating: \", input_rating)\n",
    "\n",
    "    # Learned embeddings matrix - can be initialized with pre-trained embeddings\n",
    "    word_embeddings = tf.get_variable(\n",
    "        shape=[VOCAB_SIZE + 1, EMBEDDING_SIZE], name=\"word_embeddings\", \n",
    "        dtype=tf.float32)\n",
    "    print(\"word_embeddings: \", word_embeddings)\n",
    "    \n",
    "    # Dense embedded sequence\n",
    "    embedded_sequence = tf.nn.embedding_lookup(\n",
    "        word_embeddings, input_sequence, name=\"embedded_sequence\")\n",
    "    print(\"embedded_sequence: \", embedded_sequence)\n",
    "    \n",
    "    # Convert sequence into fixed size representation for each body of text\n",
    "    # using a bidirectional LSTM\n",
    "    with tf.variable_scope('encoder_lstm'):\n",
    "        vanilla_lstm_cell_fw = tf.contrib.rnn.BasicLSTMCell(\n",
    "            num_units=64)\n",
    "        vanilla_lstm_cell_bw = tf.contrib.rnn.BasicLSTMCell(\n",
    "            num_units=64)\n",
    "    \n",
    "        vanilla_rnn_outputs, rnn_output_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=vanilla_lstm_cell_fw, cell_bw=vanilla_lstm_cell_bw, \n",
    "            inputs=embedded_sequence, dtype=tf.float32, time_major=False)\n",
    "        print(\"rnn_output_states_fw: \", rnn_output_states[0].c)\n",
    "        print(\"rnn_output_states_bw: \", rnn_output_states[1].c)\n",
    "        \n",
    "    final_lstm_output = tf.concat([rnn_output_states[0].c, rnn_output_states[1].c], axis=1)\n",
    "    print(\"final_lstm_output: \", final_lstm_output)\n",
    "    \n",
    "    # Concatenate the rating and the representation\n",
    "    conditioned_lstm_output = tf.concat([input_rating, final_lstm_output], 1)\n",
    "    print(\"conditioned_lstm_output: \", conditioned_lstm_output)\n",
    "    \n",
    "    bootstrapping_word = tf.zeros_like(conditioned_lstm_output)\n",
    "#     print(\"bootstrapping_word: \", bootstrapping_word)\n",
    "\n",
    "    sequence_indices = list()\n",
    "    for i in range(MAX_SEQUENCE_LENGTH):\n",
    "        joint_embedding = tf.concat([conditioned_lstm_output, bootstrapping_word], 1)\n",
    "        \n",
    "        bootstrapping_word = tf.layers.dense(\n",
    "            inputs=joint_embedding, units=bootstrapping_word.shape[1], \n",
    "            activation=tf.nn.relu)\n",
    "#         print(\"bootstrapping_word: \", bootstrapping_word)\n",
    "        \n",
    "        softmax_prediction = tf.layers.dense(\n",
    "            inputs=bootstrapping_word, units=VOCAB_SIZE, \n",
    "            activation=tf.nn.sigmoid)\n",
    "#         print(\"softmax_prediction: \", softmax_prediction)\n",
    "        \n",
    "        word_index_prediction = tf.argmax(softmax_prediction, axis=1)\n",
    "        sequence_indices.append(word_index_prediction)\n",
    "        \n",
    "    sequence_indices = tf.concat(sequence_indices, axis=0)\n",
    "    print(\"sequence_indices: \", sequence_indices)\n",
    "\n",
    "\n",
    "#     def perform_vocab_softmax(word_tensor):\n",
    "#         dense_word_1 = tf.layers.dense(\n",
    "#             inputs=word_tensor, units=VOCAB_SIZE, \n",
    "#             activation=tf.nn.relu, name=\"dense_word_1\")\n",
    "#         return dense_word_1\n",
    "        \n",
    "#     mapped_lstm_output = tf.map_fn(\n",
    "#         perform_vocab_softmax,\n",
    "#         vanilla_rnn_outputs[0],\n",
    "#         name='mapped_lstm'\n",
    "#     )\n",
    "#     print(\"mapped_lstm_output: \", mapped_lstm_output)\n",
    "    \n",
    "    generator_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "        logits=input_sequence,\n",
    "        targets=input_sequence, \n",
    "        weights=tf.ones_like(\n",
    "            input_sequence, dtype=tf.float32, name=None, optimize=True\n",
    "        ),\n",
    "        name='generator_loss'\n",
    "    )\n",
    "    print(\"generator_loss: \", generator_loss)\n",
    "    \n",
    "#     generator_loss_summary = tf.summary.scalar(\n",
    "#         \"generated-sequence-loss-\" + str(current_epoch), tf.convert_to_tensor(generator_loss))\n",
    "    \n",
    "#     generator_optimizer = tf.train.AdamOptimizer()\n",
    "#     generator_train_operation = generator_optimizer.minimize(generator_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph_1) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(\n",
    "        logdir=\"/home/v2john/tensorlogs/\", graph=graph_1)\n",
    "\n",
    "    epoch_reporting_interval = 10\n",
    "    training_examples_fraction = 0.9\n",
    "    training_examples_size = int(training_examples_fraction * len(labels))\n",
    "    batch_size = 50\n",
    "    training_epochs = 100\n",
    "    num_batches = int(training_examples_size/batch_size)\n",
    "\n",
    "    mini_epoch = 1\n",
    "    loss_var = None\n",
    "    loss_summary_var = None\n",
    "    for current_epoch in range(1, training_epochs + 1):\n",
    "        for batch_number in range(num_batches):\n",
    "            _, loss_var, loss_summary_var = sess.run(\n",
    "                [train_op, loss, loss_summary], \n",
    "                feed_dict={\n",
    "                    input_x: indexed_sequences[batch_number * batch_size : \n",
    "                                               (batch_number + 1) * batch_size],\n",
    "                    input_y: labels[batch_number * batch_size : \n",
    "                                    (batch_number + 1) * batch_size]})\n",
    "            writer.add_summary(loss_summary_var, mini_epoch)\n",
    "            writer.flush()\n",
    "            mini_epoch += 1\n",
    "\n",
    "        if (current_epoch % epoch_reporting_interval == 0):\n",
    "            print(\"Training epoch:\", current_epoch, \", Loss:\", loss_var)\n",
    "\n",
    "    training_predictions = sess.run(\n",
    "        prediction, \n",
    "        feed_dict={\n",
    "            input_x: indexed_sequences[:training_examples_size], \n",
    "            input_y: labels[:training_examples_size]\n",
    "        })\n",
    "\n",
    "    test_predictions = sess.run(\n",
    "        prediction, \n",
    "        feed_dict={\n",
    "            input_x: indexed_sequences[training_examples_size:], \n",
    "            input_y: labels[training_examples_size:]\n",
    "        })\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
